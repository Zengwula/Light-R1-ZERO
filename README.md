# Light-R1-ZERO
本项目是笔者学习R1论文时，对DeepSeek R1-zero进行一个轻量级的复现。因为笔者的设备仅有RTX4070 Laptop，8GB的显存，所以本项目主要面对同样显存不足，但是也渴望动手学习大模型的同学。

# 使用官方方法复现的一些问题
DeepSeekR1官方的技术报告中指出，其R1-Zero的训练，是直接在V3模型基础上应用GRPO，且得出结论直接使用RL效果要更好。但是笔者在复现时候发现了一些问题，其实这些问题也和显存有关，显存有关问题在下一节我们详细讨论。

现在的问题是，为了能够复现GRPO且显存不被爆掉，在基座模型上就只能选择较小的模型，笔者在复现时选用的是QWen2.5-0.5B的模型，并在GSM8K数据集上来进行复现。但是GRPO会有一个组内比较的过程，就是表现相对较好的，会被赋予更好的优势。但是小模型有一个显著的问题，就是他的能力仅仅是堪堪回答一些对话问题，在GSM8K测试集上的正确率仅有3%。这就直接导致了GRPO的组内的几个答案大概率没有一个是正确答案，表现相对较好的答案或者说优势更高的答案可能依然是个错误答案，这就会使模型朝着错误的方向优化，且对奖励函数的设计提出了几乎严苛的要求，训练过程中模型大概率会跑歪。另外一个问题是小模型难以读懂给他的think，answer这种标签，就导致在奖励函数里面有个格式对了就给分的模块，这个分始终给不出去。

因此，笔者在项目中还是决定，先用SFT把模型弄成至少是“能用”的状态，再进行强化学习。所以在本项目中主要有三个部分，一是全量微调，二是LoRA，三是GRPO。

# 显存计算
为了最大幅度发挥各位手中的显存资源，方便各位选择基座模型，我们先从显存占用分析入手，这也是项目中带来问题最多的部分。

首先先来分析全量微调的显存占用，显存主要有模型参数、梯度、优化器状态、激活值这四个部分。我们在估算时候只需要把这些大头都算个差不多，到时候在调参让显存不爆掉就行。

考虑半精度或者混合精度计算，模型参数一般是2字节，2*0.5B=1，所以模型参数主要占1GB。然后是梯度，因为每个参数都会有一个梯度，所以梯度大小和模型参数一致，也是1GB。

下面是优化器状态，这里主要使用adam优化器，它会存储参数副本、动量和方差三个参数，且都是32位浮点数。所以消耗的显存是0.5B×12×4字节=5.6GB.

激活值占用为batch_size × seq_len × hidden_size × 层数 × 估算系数，这个要算起来还比较麻烦，好消息是我们可以通过调参来控制，所以在实验中来动态调参控制显存占用即可。

全量微调对显存消耗巨大，相比之下LoRA微调则可以有效降低显存消耗，且减少训练时间。总结LoRA的话就是用五分之一的训练时间训练出等效全量微调一半效果的模型。对LoRA微调来说，因为只需要更新AB两个小矩阵，通过调整rank参数来调整参数量，所以在梯度和优化器状态这两部分，相比于全量微调可以显著降低，而在模型参数和激活值占用则和全量微调差不多。

下面来考虑一下GRPO算法的显存占用。在本项目中主要使用trl库中复现的GRPOtrainer。在PPO中，主要需要四个模型，分别是策略模型(actor)、价值模型(critic)、参考模型以及奖励模型。相比于PPO，GRPO直接移除了critic模块，只训练actor策略模型。critic模块的职责则通过组内相对优势作为基线。同样的，策略模型中参数占掉1GB，梯度1GB，优化器参数5.6GB，这些大头跟全量微调都差不多。对于参考模型，主要在GRPO中用于计算KL散度，在GRPO中对KL散度的计算进行了优化，只取了一个动作的概率来计算KL散度，这样就不需要维护整个动作空间，显存占用也大大减少。对于奖励模型，项目中为了简单起见，选择提前定义好奖励规则，如输出包含think和answer等模块可以得到分数，结果正确得到分数等。

# 项目架构
项目总共提供了全量微调训练代码，LoRA训练代码和GRPO训练代码，供大家参考！



