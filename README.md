# Light-R1-ZERO
本项目是笔者学习R1论文时，对DeepSeek R1-zero进行一个轻量级的复现。因为笔者的设备仅有RTX4070 Laptop，8GB的显存，所以本项目主要面对同样显存不足，但是也渴望动手学习大模型的同学。

# 使用官方方法复现的一些问题
DeepSeekR1官方的技术报告中指出，其R1-Zero的训练，是直接在V3模型基础上应用GRPO，且得出结论直接使用RL效果要更好。但是笔者在复现时候发现了一些问题，其实这些问题也和显存有关，显存有关问题在下一节我们详细讨论。

现在的问题是，为了能够复现GRPO且显存不被爆掉，在基座模型上就只能选择较小的模型，笔者在复现时选用的是QWen2.5-0.5B的模型，并在GSM8K数据集上来进行复现。但是GRPO会有一个组内比较的过程，就是表现相对较好的，会被赋予更好的优势。但是小模型有一个显著的问题，就是他的能力仅仅是堪堪回答一些对话问题，在GSM8K测试集上的正确率仅有3%。这就直接导致了GRPO的组内的几个答案大概率没有一个是正确答案，表现相对较好的答案或者说优势更高的答案可能依然是个错误答案，这就会使模型朝着错误的方向优化，且对奖励函数的设计提出了几乎严苛的要求，训练过程中模型大概率会跑歪。另外一个问题是小模型难以读懂给他的think，answer这种标签，就导致在奖励函数里面有个格式对了就给分的模块，这个分始终给不出去。

因此，笔者在项目中还是决定，先用SFT把模型弄成至少是“能用”的状态，再进行强化学习。所以在本项目中主要有三个部分，一是全量微调，二是LoRA，三是GRPO。

# 显存计算
为了最大幅度发挥各位手中的显存资源，方便各位选择基座模型，我们先从显存占用分析入手，这也是项目中带来问题最多的部分。

首先先来分析全量微调的显存占用，显存主要有模型参数、梯度、优化器状态、激活值这四个部分。我们在估算时候只需要把这些大头都算个差不多，到时候在调参让显存不爆掉就行。

考虑半精度或者混合精度计算，模型参数一般是2字节，2*0.5B=1，所以模型参数主要占1GB。然后是梯度，因为每个参数都会有一个梯度，所以梯度大小和模型参数一致，也是1GB。

下面是优化器状态，这里主要使用adam优化器，它会存储参数副本、动量和方差三个参数，且都是32位浮点数。所以消耗的显存是0.5B×12×4字节=5.6GB.

激活值占用为batch_size × seq_len × hidden_size × 层数 × 估算系数，这个要算起来还比较麻烦，好消息是我们可以通过调参来控制，所以在实验中来动态调参控制显存占用即可。


